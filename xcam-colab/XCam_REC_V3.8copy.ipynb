{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "451f1598",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/SamuelPassamani/XCam/blob/main/xcam-colab/XCam_REC_V3.8copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9hve1ySGVAs",
   "metadata": {
    "id": "c9hve1ySGVAs"
   },
   "source": [
    "# Célula 1: Configurações Auxiliares, Parâmetros Globais e Log Centralizado\n",
    "\n",
    "**Objetivo:**  \n",
    "Esta célula inicializa e centraliza todas as variáveis globais, parâmetros essenciais e agora também fornece um utilitário robusto para o log único do notebook XCam.  \n",
    "Permite ajuste rápido e seguro do comportamento do notebook, incluindo limites de processamento, controle de gravação, commit automático e mecanismos de resiliência contra transmissões problemáticas.\n",
    "\n",
    "## Principais pontos e melhorias implementadas\n",
    "\n",
    "- **Centralização dos parâmetros globais:**  \n",
    "  Todos os valores críticos (limites, thresholds, caminhos) são definidos e propagados como globais pelo notebook.\n",
    "- **Log único modular e estruturado (`xcam_master.log`):**  \n",
    "  Todas as operações relevantes (busca, gravação, blacklist, commit, erros, etc.) agora são registradas em um único arquivo JSON Lines.  \n",
    "  Cada entrada inclui sessão, evento, id, username, timestamps, status e detalhes.\n",
    "- **Funções utilitárias para o log:**  \n",
    "  Adição, busca, remoção e atualização de eventos são facilitadas por funções modulares (CRUD), promovendo robustez, rastreabilidade e fácil manutenção.\n",
    "- **Blacklist, falhas e processamento padronizados por `id`:**  \n",
    "  Toda lógica de controle é feita via identificador único, com `username` para exibição, garantindo unicidade e eliminando inconsistências.\n",
    "- **Função interativa para seleção de transmissões específicas:**  \n",
    "  Permite ao usuário informar nomes de usuários para filtrar transmissões antes do processamento.\n",
    "- **Comentários detalhados:**  \n",
    "  Cada etapa do código está documentada para orientar ajustes, manutenção e integração por toda a equipe.\n",
    "\n",
    "---\n",
    "\n",
    "## Parâmetros globais controlados nesta célula\n",
    "\n",
    "- **`LIMIT_DEFAULT`**: Quantidade máxima de transmissões processadas em paralelo/lote.\n",
    "- **`PAGE_DEFAULT`**: Página inicial para busca na API.\n",
    "- **`RECORD_SECONDS`**: Tempo máximo de gravação de cada vídeo (em segundos).\n",
    "- **`RECORD_SECONDS_MIN`**: Tempo mínimo exigido para considerar o vídeo válido (em segundos).\n",
    "- **`API_SEARCH_LIMIT`**: Limite de transmissões retornadas ao buscar usuários específicos.\n",
    "- **`COMMIT_PUSH_THRESHOLD`**: Quantidade de transmissões processadas até realizar commit/push automático (0 = commit imediato a cada gravação).\n",
    "- **`LOG_PATH`**: Caminho do arquivo único de log (JSONL).\n",
    "- **`BLACKLIST_TIMEOUT`**: Tempo de expiração da blacklist (em segundos).\n",
    "- **`BLACKLIST_MAX_FAILURES`**: Quantidade de falhas consecutivas antes de banir temporariamente o usuário.\n",
    "\n",
    "---\n",
    "\n",
    "## Estrutura do log único (`xcam_master.log`)\n",
    "\n",
    "Cada entrada segue o modelo:\n",
    "```json\n",
    "{\n",
    "  \"timestamp\": \"2025-06-06T06:15:00Z\",\n",
    "  \"sessao\": \"busca|gravação|blacklist|commit|erro|...\",\n",
    "  \"evento\": \"...\",\n",
    "  \"id\": \"...\",         // identificador único (primário)\n",
    "  \"username\": \"...\",   // nome do usuário para exibição\n",
    "  \"status\": \"...\",     // ok|erro|blacklisted|expirado|...\n",
    "  \"detalhes\": \"...\",   // informações adicionais\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Funções utilitárias para o log\n",
    "\n",
    "- **`append_log(entry, log_path=LOG_PATH)`**: Adiciona uma nova entrada ao log central.\n",
    "- **`read_logs(log_path=LOG_PATH)`**: Lê todas as entradas do log.\n",
    "- **`query_logs(...)`**: Consulta entradas do log por filtros opcionais (sessão, id, status, etc).\n",
    "- **`remove_logs(condition_fn, log_path=LOG_PATH)`**: Remove todas as entradas que satisfaçam a condição.\n",
    "- **`update_log_entry(match_fn, update_fn, log_path=LOG_PATH)`**: Atualiza entradas do log conforme regra.\n",
    "\n",
    "---\n",
    "\n",
    "## Exemplo de uso das funções (a serem aplicadas nas próximas células)\n",
    "\n",
    "```python\n",
    "append_log({\n",
    "    \"sessao\": \"busca\",\n",
    "    \"evento\": \"encontrado\",\n",
    "    \"id\": \"abc123\",\n",
    "    \"username\": \"Manugic_\",\n",
    "    \"status\": \"ok\",\n",
    "    \"detalhes\": \"URL válida\"\n",
    "})\n",
    "\n",
    "# Consultar blacklist:\n",
    "logs_blacklist = query_logs(sessao=\"blacklist\", status=\"blacklisted\")\n",
    "\n",
    "# Remover registros expirados:\n",
    "remove_logs(lambda entry: entry[\"sessao\"] == \"processing\" and expirou(entry), log_path=LOG_PATH)\n",
    "\n",
    "# Atualizar status:\n",
    "update_log_entry(lambda e: e[\"id\"]==\"abc123\", lambda e: e.update({\"status\":\"ok\"}))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Função interativa\n",
    "\n",
    "Permite ao usuário informar transmissões específicas a serem gravadas antes de iniciar o processamento.\n",
    "\n",
    "---\n",
    "\n",
    "## Segurança, rastreabilidade e manutenção\n",
    "\n",
    "- Todos os parâmetros globais são definidos no início e propagados para todo o notebook, garantindo consistência.\n",
    "- O log único fornece rastreabilidade detalhada e elimina arquivos dispersos (blacklist, falha, etc).\n",
    "- Ajuste qualquer valor diretamente nesta célula para alterar o comportamento global do notebook de forma segura.\n",
    "- Comentários detalhados auxiliam a compreensão, integração e manutenção por toda a equipe.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j5pPh353GLMD",
   "metadata": {
    "id": "j5pPh353GLMD"
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Célula 1: Configuração Global, Parâmetros e Utilitário de Log Único\n",
    "# ================================================================\n",
    "# Objetivo:\n",
    "# - Centralizar configurações globais e thresholds\n",
    "# - Definir e montar caminhos do notebook\n",
    "# - Fornecer utilitário robusto para LOG ÚNICO MODULAR (JSONL)\n",
    "#   => Todas as células e funções usarão este log para registrar, consultar e manipular eventos\n",
    "# - Garantir padronização, rastreabilidade e fácil manutenção futura\n",
    "#\n",
    "# Estratégia aplicada (conforme plano):\n",
    "# - Log único estruturado (JSONL): sessão, evento, id, username, timestamps, status, detalhes\n",
    "# - Funções CRUD para log: adicionar, buscar, atualizar, remover (para blacklist, processing, falhas, auditoria)\n",
    "# - Blacklist e controles baseados em id (com username apenas para exibição)\n",
    "# - Parâmetros globais facilmente editáveis e propagados via globals()\n",
    "# ================================================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ============================\n",
    "# PARÂMETROS GLOBAIS EDITÁVEIS\n",
    "# ============================\n",
    "# Modifique abaixo conforme necessidade do ambiente ou processamento\n",
    "\n",
    "# Limites e thresholds principais de processamento\n",
    "LIMIT_DEFAULT = 50             # Máximo de transmissões processadas por rodada\n",
    "PAGE_DEFAULT = 1               # Página padrão para busca na API\n",
    "RECORD_SECONDS = 12780         # Duração máxima da gravação (em segundos)\n",
    "RECORD_SECONDS_MIN = 660       # Duração mínima válida (em segundos)\n",
    "API_SEARCH_LIMIT = 1500        # Limite ao buscar usuários específicos\n",
    "COMMIT_PUSH_THRESHOLD = 25     # Quantidade de transmissões até commit/push automático (0 = commit imediato)\n",
    "\n",
    "# Caminhos de arquivos principais\n",
    "BASE_PATH = '/content'\n",
    "LOG_PATH = f\"{BASE_PATH}/xcam_master.log\"          # Arquivo único de log central\n",
    "BLACKLIST_TIMEOUT = 15 * 60                        # Blacklist: tempo de expiração (segundos)\n",
    "BLACKLIST_MAX_FAILURES = 3                         # Blacklist: falhas para banimento temporário\n",
    "\n",
    "# ============================\n",
    "# ATUALIZAÇÃO GLOBAL DOS PARÂMETROS\n",
    "# ============================\n",
    "# Propaga parâmetros como globais do notebook\n",
    "globals().update({\n",
    "    'LIMIT_DEFAULT': LIMIT_DEFAULT,\n",
    "    'PAGE_DEFAULT': PAGE_DEFAULT,\n",
    "    'RECORD_SECONDS': RECORD_SECONDS,\n",
    "    'RECORD_SECONDS_MIN': RECORD_SECONDS_MIN,\n",
    "    'API_SEARCH_LIMIT': API_SEARCH_LIMIT,\n",
    "    'COMMIT_PUSH_THRESHOLD': COMMIT_PUSH_THRESHOLD,\n",
    "    'LOG_PATH': LOG_PATH,\n",
    "    'BLACKLIST_TIMEOUT': BLACKLIST_TIMEOUT,\n",
    "    'BLACKLIST_MAX_FAILURES': BLACKLIST_MAX_FAILURES\n",
    "})\n",
    "\n",
    "# =============================================================================\n",
    "# UTILITÁRIO DE LOG ÚNICO MODULAR (JSONL)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cada entrada: {\n",
    "#   \"timestamp\": \"2025-06-06T06:15:00Z\",\n",
    "#   \"sessao\": \"busca|gravação|blacklist|commit|erro|...\",\n",
    "#   \"evento\": \"...\",\n",
    "#   \"id\": \"...\",         # sempre o identificador primário!\n",
    "#   \"username\": \"...\",   # para exibição/auditoria\n",
    "#   \"status\": \"...\",     # ok|erro|blacklisted|expirado|...\n",
    "#   \"detalhes\": \"...\",   # info extra (motivo, paths, etc)\n",
    "# }\n",
    "# =============================================================================\n",
    "\n",
    "def now_iso():\n",
    "    \"\"\"Retorna timestamp UTC em formato ISO.\"\"\"\n",
    "    from datetime import datetime\n",
    "    return datetime.utcnow().isoformat() + \"Z\"\n",
    "\n",
    "def append_log(entry, log_path=LOG_PATH):\n",
    "    \"\"\"\n",
    "    Adiciona uma nova entrada ao log central (JSONL).\n",
    "    Campos obrigatórios: sessao, evento, id, username, status.\n",
    "    \"\"\"\n",
    "    entry.setdefault(\"timestamp\", now_iso())\n",
    "    # Garante campos essenciais para rastreabilidade\n",
    "    for field in [\"sessao\", \"evento\", \"id\", \"username\", \"status\"]:\n",
    "        entry.setdefault(field, \"\")\n",
    "    with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def read_logs(log_path=LOG_PATH):\n",
    "    \"\"\"Lê todas as entradas do log central.\"\"\"\n",
    "    if not os.path.exists(log_path):\n",
    "        return []\n",
    "    with open(log_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "def query_logs(sessao=None, id=None, username=None, evento=None, status=None, after=None, before=None, log_path=LOG_PATH):\n",
    "    \"\"\"\n",
    "    Consulta entradas do log por filtros opcionais.\n",
    "    - after/before: string ISO ou datetime\n",
    "    \"\"\"\n",
    "    logs = read_logs(log_path)\n",
    "    result = []\n",
    "    for entry in logs:\n",
    "        if sessao and entry.get(\"sessao\") != sessao:\n",
    "            continue\n",
    "        if id and entry.get(\"id\") != id:\n",
    "            continue\n",
    "        if username and entry.get(\"username\") != username:\n",
    "            continue\n",
    "        if evento and entry.get(\"evento\") != evento:\n",
    "            continue\n",
    "        if status and entry.get(\"status\") != status:\n",
    "            continue\n",
    "        ts = entry.get(\"timestamp\")\n",
    "        if after:\n",
    "            after_val = after if isinstance(after, str) else after.isoformat()\n",
    "            if ts < after_val:\n",
    "                continue\n",
    "        if before:\n",
    "            before_val = before if isinstance(before, str) else before.isoformat()\n",
    "            if ts > before_val:\n",
    "                continue\n",
    "        result.append(entry)\n",
    "    return result\n",
    "\n",
    "def remove_logs(condition_fn, log_path=LOG_PATH):\n",
    "    \"\"\"\n",
    "    Remove do log central todas as entradas que satisfaçam condition_fn(entry).\n",
    "    Útil para expurgar logs expirados, blacklists vencidas, eventos processados, etc.\n",
    "    \"\"\"\n",
    "    logs = read_logs(log_path)\n",
    "    kept = [entry for entry in logs if not condition_fn(entry)]\n",
    "    with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in kept:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "    return len(logs) - len(kept)\n",
    "\n",
    "def update_log_entry(match_fn, update_fn, log_path=LOG_PATH):\n",
    "    \"\"\"\n",
    "    Atualiza entradas do log central: se match_fn(entry)==True, aplica update_fn(entry).\n",
    "    Exemplo: promover status de \"pending\" para \"ok\".\n",
    "    \"\"\"\n",
    "    logs = read_logs(log_path)\n",
    "    updated = 0\n",
    "    for entry in logs:\n",
    "        if match_fn(entry):\n",
    "            update_fn(entry)\n",
    "            updated += 1\n",
    "    with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in logs:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "    return updated\n",
    "\n",
    "# Exemplos de uso (para as próximas células):\n",
    "# append_log({\"sessao\":\"busca\", \"evento\":\"encontrado\", \"id\":\"abc123\", \"username\":\"Manugic_\", \"status\":\"ok\", \"detalhes\":\"URL válida\"})\n",
    "# logs_blacklist = query_logs(sessao=\"blacklist\", status=\"blacklisted\")\n",
    "# remove_logs(lambda entry: entry[\"sessao\"]==\"processing\" and expirou(entry), log_path=LOG_PATH)\n",
    "\n",
    "# =============================================================================\n",
    "# FUNÇÃO INTERATIVA (opcional) PARA ESCOLHA DE TRANSMISSÕES ESPECÍFICAS\n",
    "# =============================================================================\n",
    "def perguntar_transmissoes_especificas():\n",
    "    \"\"\"\n",
    "    Pergunta ao usuário se deseja informar transmissões específicas para gravar,\n",
    "    recebendo nomes de usuário separados por vírgula e retornando lista limpa.\n",
    "    Retorna lista vazia caso não deseje selecionar usuários.\n",
    "    \"\"\"\n",
    "    resp = input('Deseja gravar alguma transmissão específica? (sim/não): ').strip().lower()\n",
    "    if resp.startswith('s'):\n",
    "        usuarios = input('Informe o(s) nome(s) de usuário, separados por vírgula (ex: userNovo234, jovemPT): ')\n",
    "        usuarios_lista = [u.strip() for u in usuarios.split(',') if u.strip()]\n",
    "        return usuarios_lista\n",
    "    return []\n",
    "\n",
    "# =============================================================================\n",
    "# DICA DE USO EM OUTRAS CÉLULAS:\n",
    "# - Para registrar evento: append_log({...})\n",
    "# - Para consultar blacklist: query_logs(sessao=\"blacklist\", status=\"blacklisted\")\n",
    "# - Para remover registros expirados: remove_logs(lambda e: ...)\n",
    "# - Para atualizar status: update_log_entry(lambda e: ..., lambda e: ...)\n",
    "# =============================================================================\n",
    "\n",
    "# ============================\n",
    "# FIM DA CÉLULA 1\n",
    "# ============================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WXs0o6OPHXbi",
   "metadata": {
    "id": "WXs0o6OPHXbi"
   },
   "source": [
    "# Célula 2: Instalação e Validação do ffmpeg\n",
    "\n",
    "**Objetivo:**  \n",
    "Esta célula garante que o utilitário `ffmpeg` esteja instalado e disponível no ambiente Google Colab. O ffmpeg é indispensável para a gravação dos vídeos das transmissões e para o processamento de mídia ao longo do pipeline do notebook XCam.\n",
    "\n",
    "## Pontos principais e melhorias implementadas\n",
    "\n",
    "- **Verificação pré-instalação:**  \n",
    "  Antes de instalar, verifica se o ffmpeg já está disponível no ambiente, tornando o processo idempotente e eficiente.\n",
    "- **Instalação automatizada:**  \n",
    "  Efetua a instalação via `apt-get` apenas se necessário, reduzindo o tempo de setup em execuções futuras.\n",
    "- **Validação pós-instalação:**  \n",
    "  Exibe a versão instalada do ffmpeg, garantindo transparência e rastreabilidade.\n",
    "- **Mensagens detalhadas:**  \n",
    "  O usuário recebe logs informativos sobre cada etapa, facilitando o diagnóstico em caso de erros.\n",
    "- **Design modular:**  \n",
    "  Estrutura pronta para ser utilizada em outros ambientes (Colab, local, server) com pequenas adaptações.\n",
    "\n",
    "---\n",
    "\n",
    "## Como funciona a célula\n",
    "\n",
    "- **Verifica se o ffmpeg está instalado (no PATH do sistema).**\n",
    "- **Se não estiver, instala automaticamente via apt-get.**\n",
    "- **Valida e exibe a versão instalada após o processo.**\n",
    "- **Em caso de falha, exibe erro detalhado e interrompe o fluxo para evitar inconsistências futuras.**\n",
    "\n",
    "---\n",
    "\n",
    "## Exemplo de uso das funções nesta célula\n",
    "\n",
    "```python\n",
    "if not is_ffmpeg_installed():\n",
    "    install_ffmpeg()\n",
    "show_ffmpeg_version()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Segurança, rastreabilidade e manutenção\n",
    "\n",
    "- A célula torna o setup do ambiente mais robusto, impedindo falhas silenciosas relacionadas à ausência de ffmpeg.\n",
    "- Mensagens e validações ajudam a equipe a identificar rapidamente problemas de ambiente ou permissões.\n",
    "- O padrão modular facilita a reutilização do código em diferentes notebooks ou pipelines do projeto XCam.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vIODn0c2HiHz",
   "metadata": {
    "id": "vIODn0c2HiHz"
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Célula 2: Instalação e Validação do FFMPEG no Colab\n",
    "# ================================================================\n",
    "# Objetivo:\n",
    "# - Garantir que o utilitário ffmpeg está instalado e disponível no ambiente\n",
    "# - Validar a instalação e exibir a versão instalada\n",
    "# - Tornar a etapa idempotente, evitando instalações desnecessárias\n",
    "# - Fornecer feedback claro e orientações em caso de erro\n",
    "#\n",
    "# Estratégia aplicada:\n",
    "# - Instalação via apt-get apenas se ffmpeg não estiver disponível\n",
    "# - Validação pós-instalação\n",
    "# - Logs claros e comentários detalhados para rastreabilidade\n",
    "# ================================================================\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def is_ffmpeg_installed():\n",
    "    \"\"\"\n",
    "    Verifica se o ffmpeg está instalado e disponível no PATH do sistema.\n",
    "    Retorna True se estiver, False caso contrário.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = subprocess.run([\"ffmpeg\", \"-version\"], capture_output=True, text=True)\n",
    "        return result.returncode == 0\n",
    "    except FileNotFoundError:\n",
    "        return False\n",
    "\n",
    "def install_ffmpeg():\n",
    "    \"\"\"\n",
    "    Instala o ffmpeg via apt-get caso não esteja presente.\n",
    "    \"\"\"\n",
    "    print(\"[INFO] Instalando ffmpeg via apt-get...\")\n",
    "    # Atualiza pacotes e instala ffmpeg silenciosamente\n",
    "    !apt-get update -y > /dev/null\n",
    "    !apt-get install -y ffmpeg > /dev/null\n",
    "    print(\"[INFO] ffmpeg instalado com sucesso.\")\n",
    "\n",
    "def show_ffmpeg_version():\n",
    "    \"\"\"\n",
    "    Exibe a versão instalada do ffmpeg.\n",
    "    \"\"\"\n",
    "    print(\"[INFO] Versão do ffmpeg instalada:\")\n",
    "    !ffmpeg -version | head -n 2\n",
    "\n",
    "# ============================\n",
    "# EXECUÇÃO DA ETAPA DE SETUP\n",
    "# ============================\n",
    "\n",
    "if not is_ffmpeg_installed():\n",
    "    print(\"[WARN] ffmpeg não encontrado no ambiente.\")\n",
    "    install_ffmpeg()\n",
    "    if not is_ffmpeg_installed():\n",
    "        raise RuntimeError(\"[ERRO] Falha ao instalar o ffmpeg. Verifique permissões ou tente novamente.\")\n",
    "else:\n",
    "    print(\"[OK] ffmpeg já está instalado no ambiente.\")\n",
    "\n",
    "# Validação final e exibição da versão\n",
    "show_ffmpeg_version()\n",
    "\n",
    "# ============================\n",
    "# FIM DA CÉLULA 2\n",
    "# ============================\n",
    "\n",
    "# Dica: ffmpeg deve estar disponível para todas as células subsequentes.\n",
    "# Se precisar de um caminho específico, utilize `which ffmpeg` para obter o path absoluto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90qvXC0rHtWb",
   "metadata": {
    "id": "90qvXC0rHtWb"
   },
   "source": [
    "# Célula 3: Imports Essenciais, Utilitários e Preparação do Ambiente\n",
    "\n",
    "**Objetivo:**  \n",
    "Importa todas as bibliotecas essenciais do Python necessárias para o funcionamento do notebook, incluindo módulos para requisições HTTP, processamento paralelo, manipulação de datas, controle de subprocessos e exibição interativa.  \n",
    "Centraliza funções utilitárias robustas e padronizadas para processamento, download de poster, geração automática de poster com ffmpeg e exibição de progresso.  \n",
    "\n",
    "## Principais pontos e melhorias implementadas\n",
    "\n",
    "- **Centralização de imports essenciais:**  \n",
    "  Todos os módulos fundamentais (os, requests, multiprocessing, datetime, json, time, subprocess, math, re, IPython) estão disponíveis e prontos para uso global.\n",
    "- **Funções utilitárias padronizadas:**  \n",
    "  Funções para formatação de segundos, exibição de progresso, download e validação de poster e geração de poster via ffmpeg foram refatoradas e documentadas, seguindo arquitetura modular e Clean Architecture.\n",
    "- **Remoção de logs temporários dispersos:**  \n",
    "  O antigo arquivo de log de processamento temporário foi descontinuado em favor do log único centralizado definido na Célula 1, promovendo rastreabilidade e controle total.\n",
    "- **Robustez e clareza:**  \n",
    "  Todas as funções possuem tratamento de erros, mensagens amigáveis e são preparadas para uso concorrente e integração com as próximas etapas do pipeline.\n",
    "- **Pronto para uso em todo o notebook:**  \n",
    "  As funções aqui definidas são utilizadas em toda a automação, garantindo reuso, legibilidade e manutenção facilitada.\n",
    "\n",
    "---\n",
    "\n",
    "## Funções utilitárias disponíveis nesta célula\n",
    "\n",
    "- **`format_seconds(seconds)`**: Formata um valor em segundos para string legível (ex: \"1h23m45s\").\n",
    "- **`log_progress(username, elapsed_seconds, total_seconds)`**: Exibe o progresso da gravação de cada transmissão.\n",
    "- **`download_and_save_poster(poster_url, username, temp_folder)`**: Baixa e salva o poster da transmissão a partir de uma URL remota ou retorna se for um caminho local.\n",
    "- **`generate_poster_with_ffmpeg(m3u8_url, username, temp_folder, frame_time=7, timeout=20)`**: Gera automaticamente um poster usando ffmpeg, após validar a disponibilidade do stream.\n",
    "- **`is_poster_valid(poster_path)`**: Verifica se o arquivo de poster é válido (existe e não está vazio).\n",
    "\n",
    "---\n",
    "\n",
    "## Exemplo de uso das funções\n",
    "\n",
    "```python\n",
    "# Formatar segundos em string legível\n",
    "tempo = format_seconds(385)\n",
    "# Exibir progresso\n",
    "log_progress(\"userNovo234\", 385, 12780)\n",
    "# Download do poster\n",
    "poster_path = download_and_save_poster(url_poster, \"userNovo234\", \"/content/temp\")\n",
    "# Geração automática de poster via ffmpeg (se necessário)\n",
    "if not is_poster_valid(poster_path):\n",
    "    poster_path = generate_poster_with_ffmpeg(m3u8_url, \"userNovo234\", \"/content/temp\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Segurança, rastreabilidade e manutenção\n",
    "\n",
    "- Todas as funções são preparadas para tratamento de erros e integração com processos concorrentes.\n",
    "- O log temporário de processamento foi removido, garantindo que todo o rastreio e auditoria sejam feitos via log único centralizado da Célula 1.\n",
    "- Comentários detalhados facilitam manutenção, entendimento e evolução do notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hpRIMtyFIY0q",
   "metadata": {
    "id": "hpRIMtyFIY0q"
   },
   "source": [
    "# Célula 4: Clonagem do Repositório GitHub no Colab e Google Drive\n",
    "\n",
    "**Objetivo:**  \n",
    "Esta célula garante que o repositório do projeto XCam seja sempre clonado de forma limpa e sincronizada no ambiente local do Colab e, se disponível, também no Google Drive para persistência.  \n",
    "Assegura ambiente pronto, atualizado, seguro para gravações e processamento, e prepara diretórios padronizados para integração com o restante do pipeline.\n",
    "\n",
    "## Principais pontos e melhorias implementadas\n",
    "\n",
    "- **Clonagem idempotente e limpa:**  \n",
    "  Remove repositórios antigos antes de clonar para evitar conflitos, arquivos órfãos ou problemas de sincronização.\n",
    "- **Clonagem para ambiente temporário e persistente:**  \n",
    "  O repositório é clonado tanto para `/content` (Colab) quanto para o Drive (`/content/drive/MyDrive/XCam.Drive`) se o Drive estiver montado.\n",
    "- **Preparação de diretórios de gravação e processamento:**  \n",
    "  Estrutura de diretórios temporários criada automaticamente, garantindo organização dos dados.\n",
    "- **Exportação de variáveis globais:**  \n",
    "  Todos os caminhos, URLs e configurações relevantes são disponibilizados via `globals().update()` para uso em todo o notebook.\n",
    "- **Mensagens e validações detalhadas:**  \n",
    "  Feedback informativo sobre o status de cada etapa, facilitando o diagnóstico e a manutenção.\n",
    "- **Pronto para CI/CD e integrações futuras:**  \n",
    "  Token e URLs preparados para automações, integrações externas e uploads (Abyss.to, etc).\n",
    "\n",
    "---\n",
    "\n",
    "## Parâmetros globais definidos nesta célula\n",
    "\n",
    "- **`GITHUB_USER`**, **`GITHUB_REPO`**, **`GITHUB_BRANCH`**, **`GITHUB_TOKEN`**: Configurações do repositório e autenticação.\n",
    "- **`repo_url`**: URL do repositório autenticada para clone/push.\n",
    "- **`TEMP_OUTPUT_FOLDER`**: Pasta para gravações temporárias.\n",
    "- **`BASE_REPO_FOLDER`**: Localização do repositório no ambiente Colab.\n",
    "- **`DRIVE_MOUNT`**, **`DRIVE_REPO_FOLDER`**: Caminhos no Google Drive para persistência (se montado).\n",
    "- **`ABYSS_UPLOAD_URL`**: URL de upload para integração com sistemas externos.\n",
    "\n",
    "---\n",
    "\n",
    "## Como funciona a célula\n",
    "\n",
    "- **Remove repositórios antigos e diretórios temporários**, evitando resíduos de execuções anteriores.\n",
    "- **Clona o repositório do GitHub** para `/content` (Colab).\n",
    "- **Se o Google Drive estiver montado**, faz o mesmo clone no diretório persistente do Drive.\n",
    "- **Cria diretórios temporários necessários** para gravações e arquivos intermediários.\n",
    "- **Exporta todas as variáveis configuradas** para uso global no notebook.\n",
    "- **Exibe mensagens informativas** sobre cada etapa e alerta caso o Drive não esteja disponível.\n",
    "\n",
    "---\n",
    "\n",
    "## Exemplo de uso das variáveis globais\n",
    "\n",
    "```python\n",
    "print(BASE_REPO_FOLDER)        # Caminho do repositório clonado no Colab\n",
    "print(DRIVE_REPO_FOLDER)      # Caminho do repositório no Drive (se montado)\n",
    "print(TEMP_OUTPUT_FOLDER)     # Pasta temporária para gravações\n",
    "print(ABYSS_UPLOAD_URL)       # URL de upload para integração externa\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Segurança, rastreabilidade e manutenção\n",
    "\n",
    "- Garantia de ambiente limpo a cada execução, evitando conflitos de arquivos e branches.\n",
    "- Persistência dos dados no Drive (se montado), evitando perda de gravações em caso de reinicialização do Colab.\n",
    "- Comentários detalhados e estrutura modular facilitam a manutenção, integração com CI/CD e futuras expansões no pipeline do XCam.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Uof_0QCrIlf7",
   "metadata": {
    "id": "Uof_0QCrIlf7"
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Célula 4: Clonagem do Repositório GitHub no Colab e no Google Drive\n",
    "# ================================================================\n",
    "# Objetivo:\n",
    "# - Garantir ambiente limpo e sincronizado para o repositório XCam em todas as execuções\n",
    "# - Clonar o repositório tanto para o ambiente efêmero do Colab quanto para o Google Drive (persistência)\n",
    "# - Preparar diretórios de trabalho para gravações e processamento temporário\n",
    "# - Fornecer feedback claro sobre o status da operação\n",
    "#\n",
    "# Estratégia aplicada:\n",
    "# - Remove repositórios antigos antes de clonar (evita conflitos e arquivos órfãos)\n",
    "# - Utiliza token pessoal para autenticação segura e push futuro (CI/CD)\n",
    "# - Cria estrutura de diretórios padronizada (módulos, gravações, cache, etc.)\n",
    "# - Valida se o Drive está montado antes de tentar operações persistentes\n",
    "# - Comentários detalhados para fácil manutenção e evolução\n",
    "# ================================================================\n",
    "\n",
    "# ============================\n",
    "# CONFIGURAÇÕES DO GITHUB\n",
    "# ============================\n",
    "GITHUB_USER = \"SamuelPassamani\"\n",
    "GITHUB_REPO = \"XCam\"\n",
    "GITHUB_BRANCH = \"main\"\n",
    "GITHUB_TOKEN = \"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\"\n",
    "\n",
    "repo_url = f\"https://{GITHUB_USER}:{GITHUB_TOKEN}@github.com/{GITHUB_USER}/{GITHUB_REPO}.git\"\n",
    "\n",
    "# ============================\n",
    "# CLONAGEM PARA O COLAB\n",
    "# ============================\n",
    "print(f\"⏳ Limpando ambiente e clonando '{GITHUB_REPO}' para o Colab...\")\n",
    "!rm -rf {GITHUB_REPO}\n",
    "!git clone -b {GITHUB_BRANCH} {repo_url}\n",
    "print(f\"✅ Repositório clonado em /content/{GITHUB_REPO}\")\n",
    "\n",
    "# ============================\n",
    "# ESTRUTURA DE DIRETÓRIOS TEMPORÁRIOS\n",
    "# ============================\n",
    "TEMP_OUTPUT_FOLDER = \"/content/temp_recordings\"  # Para gravações temporárias\n",
    "BASE_REPO_FOLDER = f\"/content/{GITHUB_REPO}\"\n",
    "\n",
    "# ============================\n",
    "# CLONAGEM PARA O GOOGLE DRIVE (PERSISTÊNCIA)\n",
    "# ============================\n",
    "DRIVE_MOUNT = \"/content/drive/MyDrive/XCam.Drive\"\n",
    "DRIVE_REPO_FOLDER = f\"{DRIVE_MOUNT}/{GITHUB_REPO}\"\n",
    "\n",
    "import os\n",
    "\n",
    "if os.path.exists(DRIVE_MOUNT):\n",
    "    print(f\"⏳ Limpando repositório antigo no Drive (se existir)...\")\n",
    "    !rm -rf \"{DRIVE_REPO_FOLDER}\"\n",
    "    print(f\"⏳ Clonando '{GITHUB_REPO}' para o Drive em {DRIVE_REPO_FOLDER} ...\")\n",
    "    !git clone -b {GITHUB_BRANCH} {repo_url} \"{DRIVE_REPO_FOLDER}\"\n",
    "    print(f\"✅ Repositório também clonado no Drive: {DRIVE_REPO_FOLDER}\")\n",
    "else:\n",
    "    print(f\"⚠️ Google Drive não está montado em {DRIVE_MOUNT}.\\nℹ️ Use a célula de montagem antes de prosseguir para garantir persistência.\")\n",
    "\n",
    "# ============================\n",
    "# CONFIGURAÇÃO DE ENDPOINTS DE UPLOAD/INTEGRAÇÃO\n",
    "# ============================\n",
    "ABYSS_UPLOAD_URL = 'http://up.hydrax.net/0128263f78f0b426d617bb61c2a8ff43'\n",
    "globals().update({\n",
    "    'GITHUB_USER': GITHUB_USER,\n",
    "    'GITHUB_REPO': GITHUB_REPO,\n",
    "    'GITHUB_BRANCH': GITHUB_BRANCH,\n",
    "    'GITHUB_TOKEN': GITHUB_TOKEN,\n",
    "    'repo_url': repo_url,\n",
    "    'TEMP_OUTPUT_FOLDER': TEMP_OUTPUT_FOLDER,\n",
    "    'BASE_REPO_FOLDER': BASE_REPO_FOLDER,\n",
    "    'DRIVE_MOUNT': DRIVE_MOUNT,\n",
    "    'DRIVE_REPO_FOLDER': DRIVE_REPO_FOLDER,\n",
    "    'ABYSS_UPLOAD_URL': ABYSS_UPLOAD_URL\n",
    "})\n",
    "\n",
    "# ============================\n",
    "# FIM DA CÉLULA 4\n",
    "# ============================\n",
    "\n",
    "# Observações:\n",
    "# - Os caminhos globais são exportados via globals().update() para uso em todo o notebook.\n",
    "# - Recomenda-se sempre rodar esta célula após alterar tokens ou trocar branches para garantir ambiente limpo e sincronizado.\n",
    "# - O endpoint ABYSS_UPLOAD_URL pode ser atualizado conforme integrações futuras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M5iL_9BoIoj7",
   "metadata": {
    "id": "M5iL_9BoIoj7"
   },
   "source": [
    "# Célula 5: Commit e Push Automáticos (rec.json, posters, etc.)\n",
    "\n",
    "**Objetivo:**  \n",
    "Automatiza o processo de commit e push dos arquivos modificados (ex: rec.json, posters e demais artefatos importantes) para o repositório GitHub, garantindo rastreabilidade, atomicidade e integração contínua (CI/CD) do pipeline XCam.\n",
    "\n",
    "## Principais pontos e melhorias implementadas\n",
    "\n",
    "- **Função robusta e modular:**  \n",
    "  A função `git_commit_and_push()` aceita um caminho único (string) ou uma lista de arquivos, permitindo commit em lote e integração com estratégias de batch commit (threshold).\n",
    "- **Configuração automatizada de usuário e e-mail do git:**  \n",
    "  Garante commits válidos para rastreabilidade, auditoria e integração com pipelines automáticos.\n",
    "- **Validação de caminhos e mensagens informativas:**  \n",
    "  Apenas arquivos existentes são adicionados. Mensagens de sucesso, erro ou aviso detalhadas facilitam troubleshooting e manutenção.\n",
    "- **Compatível com commit vazio:**  \n",
    "  Permite o uso do parâmetro `--allow-empty` para garantir que o pipeline siga mesmo sem alterações detectadas, útil para sincronização e CI/CD.\n",
    "- **Push autenticado via token:**  \n",
    "  Utiliza o token pessoal fornecido nas variáveis globais para garantir push seguro e sem intervenção manual.\n",
    "- **Design pronto para integração com logs centralizados:**  \n",
    "  Recomenda-se registrar todas as ações relevantes de commit/push utilizando o log único modular definido na Célula 1.\n",
    "\n",
    "---\n",
    "\n",
    "## Parâmetros e variáveis globais utilizados\n",
    "\n",
    "- **`GITHUB_USER`**, **`GITHUB_REPO`**, **`GITHUB_TOKEN`**: Definidos nas células anteriores para autenticação e configuração do repositório.\n",
    "- **`repo_dir`**: Caminho absoluto do repositório clonado no ambiente Colab.\n",
    "- **`file_paths`**: String ou lista de arquivos a serem commitados e enviados.\n",
    "- **`commit_message`**: Mensagem do commit, customizável conforme a operação realizada.\n",
    "\n",
    "---\n",
    "\n",
    "## Como funciona a função principal\n",
    "\n",
    "- **Valida a existência do repositório local** antes de prosseguir.\n",
    "- **Aceita arquivos únicos ou múltiplos** para commit (string ou lista).\n",
    "- **Adiciona apenas arquivos existentes** ao staging, com avisos para arquivos não encontrados.\n",
    "- **Realiza commit (mesmo vazio) e push autenticado** para o repositório remoto.\n",
    "- **Emite mensagens claras** de sucesso, erro ou aviso ao longo do processo.\n",
    "\n",
    "---\n",
    "\n",
    "## Exemplo de uso típico\n",
    "\n",
    "```python\n",
    "# Commit e push de um único arquivo\n",
    "git_commit_and_push(\"data/rec.json\", \"Atualiza rec.json de gravação\")\n",
    "\n",
    "# Commit e push em lote (lista de arquivos)\n",
    "git_commit_and_push([\n",
    "    \"data/rec.json\",\n",
    "    \"posters/user1_poster.jpg\",\n",
    "    \"posters/user2_poster.jpg\"\n",
    "], \"Batch commit de múltiplos arquivos\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Segurança, rastreabilidade e manutenção\n",
    "\n",
    "- **Rastreabilidade garantida** por mensagens de commit claras e integração recomendada com o log modular (Célula 1).\n",
    "- **Atomicidade** em operações batch, evitando inconsistências de dados no repositório.\n",
    "- **Pronto para integração com pipelines CI/CD**, webhooks e controles de auditoria.\n",
    "- **Mensagens e tratamento de erros detalhados** facilitam o diagnóstico e a evolução do sistema.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aQn1G6yI6Gz",
   "metadata": {
    "id": "1aQn1G6yI6Gz"
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Célula 5: Commit e Push Automáticos (rec.json, posters, etc.)\n",
    "# ================================================================\n",
    "# Objetivo:\n",
    "# - Automatizar o processo de commit e push dos arquivos modificados (rec.json, posters, etc.) para o repositório GitHub\n",
    "# - Suportar tanto commit de arquivo único como em lote, permitindo estratégia de batch commit baseada em thresholds\n",
    "# - Garantir rastreabilidade, atomicidade e integração segura (CI/CD)\n",
    "#\n",
    "# Estratégia aplicada:\n",
    "# - Função modular e robusta, preparada para integração com logs e auditoria\n",
    "# - Permite commit vazio por segurança, evitando falhas em pipelines sincronizados\n",
    "# - Mensagens e tratamento de erros detalhados para facilitar troubleshooting\n",
    "# - Utilização de variáveis globais para caminhos, usuário e token definidos nas células anteriores\n",
    "# - Design pronto para evolução, reuso e integração com ferramentas externas (ex: webhooks, jobs, etc.)\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def git_commit_and_push(file_paths, commit_message=\"Atualiza rec.json\"):\n",
    "    \"\"\"\n",
    "    Realiza git add, commit e push dos arquivos especificados.\n",
    "    - file_paths pode ser uma string (arquivo único) ou uma lista de arquivos.\n",
    "    - commit_message é a mensagem de commit utilizada.\n",
    "\n",
    "    Estratégia:\n",
    "    - Ajusta diretório para o repositório local clonado no Colab\n",
    "    - Configura usuário e e-mail do git (necessários para CI/CD)\n",
    "    - Adiciona arquivos ao staging (aceita múltiplos arquivos)\n",
    "    - Realiza commit (permite commit vazio)\n",
    "    - Realiza push autenticado via token\n",
    "    \"\"\"\n",
    "    # ============================\n",
    "    # VALIDAÇÃO E AJUSTE DE ENTRADAS\n",
    "    # ============================\n",
    "    repo_dir = f\"/content/{GITHUB_REPO}\"\n",
    "    if not os.path.exists(repo_dir):\n",
    "        raise FileNotFoundError(f\"Repositório '{repo_dir}' não encontrado. Verifique se a célula de clonagem foi executada.\")\n",
    "    os.chdir(repo_dir)\n",
    "\n",
    "    # Aceita string ou lista de arquivos\n",
    "    if isinstance(file_paths, str):\n",
    "        file_paths = [file_paths]\n",
    "    elif not isinstance(file_paths, list):\n",
    "        raise ValueError(\"file_paths deve ser uma string ou uma lista de caminhos.\")\n",
    "\n",
    "    # ============================\n",
    "    # CONFIGURAÇÃO DO USUÁRIO GIT (CI/CD)\n",
    "    # ============================\n",
    "    subprocess.run([\"git\", \"config\", \"user.email\", \"contato@aserio.work\"], check=True)\n",
    "    subprocess.run([\"git\", \"config\", \"user.name\", \"SamuelPassamani\"], check=True)\n",
    "\n",
    "    # ============================\n",
    "    # ADIÇÃO DOS ARQUIVOS AO STAGING\n",
    "    # ============================\n",
    "    for file_path in file_paths:\n",
    "        # Verifica se o arquivo existe antes de adicionar\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"⚠️ Aviso: arquivo '{file_path}' não existe e será ignorado no commit.\")\n",
    "            continue\n",
    "        subprocess.run([\"git\", \"add\", file_path], check=True)\n",
    "\n",
    "    # ============================\n",
    "    # COMMIT (PERMITE COMMIT VAZIO)\n",
    "    # ============================\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            [\"git\", \"commit\", \"-m\", commit_message, \"--allow-empty\"],\n",
    "            check=False  # Não força erro se não houver mudanças\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro ao tentar realizar commit: {e}\")\n",
    "\n",
    "    # ============================\n",
    "    # PUSH PARA O REPOSITÓRIO REMOTO (AUTENTICADO)\n",
    "    # ============================\n",
    "    try:\n",
    "        remote_url = f\"https://{GITHUB_USER}:{GITHUB_TOKEN}@github.com/{GITHUB_USER}/{GITHUB_REPO}.git\"\n",
    "        subprocess.run(\n",
    "            [\"git\", \"push\", remote_url],\n",
    "            check=True\n",
    "        )\n",
    "        print(f\"✅ Push realizado com sucesso! ({commit_message})\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro ao tentar realizar push: {e}\")\n",
    "\n",
    "# ============================\n",
    "# FIM DA CÉLULA 5\n",
    "# ============================\n",
    "\n",
    "# Dicas e melhores práticas:\n",
    "# - Use commit_messages claros e informativos para facilitar a auditoria.\n",
    "# - Utilize a função dentro de loops ou triggers de batch para commit em lote.\n",
    "# - Integre logs das ações de commit/push usando o log único centralizado (Célula 1).\n",
    "# - Em caso de erro de autenticação, revise o token e as permissões do GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BZ4c3Uk1I7AK",
   "metadata": {
    "id": "BZ4c3Uk1I7AK"
   },
   "source": [
    "# Célula 6: Busca de Transmissões na API XCam, Blacklist Temporária, Fallback via liveInfo e Busca Inteligente/Unitária\n",
    "\n",
    "**Objetivo:**  \n",
    "Realizar a busca das transmissões ativas na API principal da XCam, mantendo o lote de transmissões sempre completo até o `LIMIT_DEFAULT` e sem duplicidades, utilizando controle de blacklist temporária e log de transmissões em processamento.  \n",
    "Inclui funções de busca unitária/inteligente (para manter “lote cheio” continuamente) e gerenciamento automático de poster, com geração via ffmpeg quando necessário.\n",
    "\n",
    "## Estratégia e melhorias implementadas\n",
    "\n",
    "- **Blacklist temporária e controle de falhas:**  \n",
    "  Usuários problemáticos são bloqueados temporariamente após atingirem o limite de falhas (`BLACKLIST_MAX_FAILURES`), acelerando o processamento e evitando ciclos infinitos.\n",
    "- **Busca em lote e unitária com fallback:**  \n",
    "  Consulta a API principal com limite alto para preencher o lote rapidamente. Caso necessário, realiza fallback via `/liveInfo` para usuários sem `src`.\n",
    "- **Controle de duplicidade e fila inteligente:**  \n",
    "  Antes de incluir qualquer transmissão, verifica no log de processamento e na blacklist para evitar tentativas repetidas ou paradas em streams problemáticos.\n",
    "- **Poster garantido:**  \n",
    "  Se o poster estiver ausente, inválido ou nulo, gera automaticamente uma imagem via ffmpeg a partir do stream, garantindo sempre um arquivo válido.\n",
    "- **Eficiência e paralelismo:**  \n",
    "  Todas as funções são preparadas para processamento paralelo e integração total ao pipeline XCam.\n",
    "- **Compatibilidade:**  \n",
    "  Suporte total à busca de usuários específicos, agora também protegida pela blacklist e controle de falhas.\n",
    "- **Design modular:**  \n",
    "  Funções separadas para busca em lote (`get_broadcasts`), busca por usuários (`buscar_usuarios_especificos`) e busca unitária/primeira transmissão livre (`buscar_proxima_transmissao_livre`), facilitando reuso e manutenção.\n",
    "\n",
    "---\n",
    "\n",
    "## Como funciona cada função\n",
    "\n",
    "- **get_broadcasts:**  \n",
    "  Retorna um lote de transmissões válidas, sempre checando blacklist, log de processamento e gerando poster se necessário. Realiza fallback automático para `/liveInfo` para usuários sem `src`.\n",
    "- **buscar_usuarios_especificos:**  \n",
    "  Busca apenas os usuários informados, respeitando sempre o controle de blacklist/falhas, e faz fallback via `/liveInfo` quando necessário.\n",
    "- **buscar_proxima_transmissao_livre:**  \n",
    "  Busca rapidamente a próxima transmissão livre para processamento, sempre utilizando os mesmos critérios de controle, garantindo agilidade na fila e eficiência máxima.\n",
    "\n",
    "---\n",
    "\n",
    "## Detalhes técnicos e recomendações\n",
    "\n",
    "- **Blacklist temporária e controle de falhas:**  \n",
    "  Funções `register_failure`, `clear_failure`, `add_to_blacklist`, `is_in_blacklist`, `load_blacklist` e `save_blacklist` garantem rastreabilidade e bloqueio eficiente de usuários problemáticos.\n",
    "- **Arquitetura limpa e modular:**  \n",
    "  Código preparado para integração futura com log único centralizado e processamento concorrente.\n",
    "- **Poster sempre válido:**  \n",
    "  Funções utilitárias garantem que cada transmissão só é liberada para gravação se houver poster válido (baixado ou gerado).\n",
    "- **Tratamento de erros robusto:**  \n",
    "  Toda etapa crítica possui tratamento de exceções e mensagens claras para facilitar manutenção e monitoramento.\n",
    "\n",
    "---\n",
    "\n",
    "## Exemplo de uso das funções\n",
    "\n",
    "```python\n",
    "# Buscar lote completo de transmissões válidas\n",
    "streams = get_broadcasts(limit=LIMIT_DEFAULT)\n",
    "\n",
    "# Buscar apenas usuários específicos\n",
    "streams_especificos = buscar_usuarios_especificos([\"user1\", \"user2\"])\n",
    "\n",
    "# Buscar a próxima transmissão livre disponível\n",
    "proxima_stream = buscar_proxima_transmissao_livre()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Rastreabilidade, manutenção e integração\n",
    "\n",
    "- Blacklist e falhas podem ser migrados para o log centralizado para máxima rastreabilidade.\n",
    "- Todas as funções são compatíveis com execução paralela e integração CI/CD.\n",
    "- Mensagens detalhadas e arquitetura modular facilitam manutenção e futuras expansões no pipeline do XCam.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h1jr7D0pJ7jS",
   "metadata": {
    "id": "h1jr7D0pJ7jS"
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Célula 6: Blacklist, Falhas, Processamento e Funções de Busca de Transmissões\n",
    "# ================================================================\n",
    "# Este arquivo une as funções de controle (blacklist, falhas, processamento)\n",
    "# e as funções de busca de transmissões, garantindo integração total.\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "import threading\n",
    "\n",
    "# Funções utilitárias importadas da célula 3 (garante que estão disponíveis)\n",
    "try:\n",
    "    download_and_save_poster\n",
    "    generate_poster_with_ffmpeg\n",
    "    is_poster_valid\n",
    "except NameError:\n",
    "    def download_and_save_poster(*a, **kw): raise NotImplementedError('Função não definida!')\n",
    "    def generate_poster_with_ffmpeg(*a, **kw): raise NotImplementedError('Função não definida!')\n",
    "    def is_poster_valid(*a, **kw): raise NotImplementedError('Função não definida!')\n",
    "\n",
    "# ============================\n",
    "# PARÂMETROS E CAMINHOS GLOBAIS (ajuste conforme necessário)\n",
    "# ============================\n",
    "BLACKLIST_PATH = \"/content/xcam_blacklist.log\"    # Preferencialmente use log centralizado!\n",
    "FAILURE_LOG_PATH = \"/content/xcam_failures.log\"   # Preferencialmente use log centralizado!\n",
    "TEMP_OUTPUT_FOLDER = \"/content/temp_recordings\"   # Ajuste conforme seu ambiente\n",
    "LIMIT_DEFAULT = 50\n",
    "API_SEARCH_LIMIT = 1500\n",
    "BLACKLIST_TIMEOUT = 15 * 60\n",
    "BLACKLIST_MAX_FAILURES = 3\n",
    "\n",
    "# ============================\n",
    "# BLACKLIST TEMPORÁRIA - CRUD\n",
    "# ============================\n",
    "def load_blacklist():\n",
    "    if not os.path.exists(BLACKLIST_PATH):\n",
    "        return {}\n",
    "    with open(BLACKLIST_PATH, \"r\") as f:\n",
    "        now = time.time()\n",
    "        lines = [line.strip().split(\",\") for line in f if line.strip()]\n",
    "        return {user: float(ts) for user, ts in lines if now - float(ts) < BLACKLIST_TIMEOUT}\n",
    "\n",
    "def save_blacklist(blacklist):\n",
    "    with open(BLACKLIST_PATH, \"w\") as f:\n",
    "        for user, ts in blacklist.items():\n",
    "            f.write(f\"{user},{ts}\\n\")\n",
    "\n",
    "def add_to_blacklist(username):\n",
    "    entry = {\n",
    "        \"sessao\": \"blacklist\",\n",
    "        \"evento\": \"add\",\n",
    "        \"id\": username,\n",
    "        \"username\": username,\n",
    "        \"status\": \"blacklisted\",\n",
    "        \"detalhes\": \"Adicionado à blacklist\",\n",
    "        \"timestamp\": now_iso()\n",
    "    }\n",
    "    append_log(entry)\n",
    "    print(f\"⚠️ Usuário '{username}' adicionado à blacklist temporária.\")\n",
    "\n",
    "def is_in_blacklist(username):\n",
    "    logs = query_logs(sessao=\"blacklist\", id=username, status=\"blacklisted\")\n",
    "    if not logs:\n",
    "        return False\n",
    "    # Verifica expiração\n",
    "    last = logs[-1]\n",
    "    from datetime import datetime, timezone\n",
    "    from dateutil import parser\n",
    "    ts = parser.isoparse(last[\"timestamp\"]).replace(tzinfo=timezone.utc).timestamp()\n",
    "    if time.time() - ts < BLACKLIST_TIMEOUT:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# ============================\n",
    "# CONTROLE DE FALHAS POR USUÁRIO\n",
    "# ============================\n",
    "def load_failures():\n",
    "    if not os.path.exists(FAILURE_LOG_PATH):\n",
    "        return {}\n",
    "    with open(FAILURE_LOG_PATH, \"r\") as f:\n",
    "        return {user: int(count) for user, count in (line.strip().split(\",\") for line in f if line.strip())}\n",
    "\n",
    "def save_failures(failures):\n",
    "    with open(FAILURE_LOG_PATH, \"w\") as f:\n",
    "        for user, count in failures.items():\n",
    "            f.write(f\"{user},{count}\\n\")\n",
    "\n",
    "def register_failure(username):\n",
    "    fails = query_logs(sessao=\"failure\", id=username)\n",
    "    count = len([f for f in fails if f.get(\"status\") == \"fail\"])\n",
    "    entry = {\n",
    "        \"sessao\": \"failure\",\n",
    "        \"evento\": \"fail\",\n",
    "        \"id\": username,\n",
    "        \"username\": username,\n",
    "        \"status\": \"fail\",\n",
    "        \"detalhes\": f\"Falha registrada. Total: {count+1}\",\n",
    "        \"timestamp\": now_iso()\n",
    "    }\n",
    "    append_log(entry)\n",
    "    if count+1 >= BLACKLIST_MAX_FAILURES:\n",
    "        add_to_blacklist(username)\n",
    "\n",
    "def clear_failure(username):\n",
    "    entry = {\n",
    "        \"sessao\": \"failure\",\n",
    "        \"evento\": \"clear\",\n",
    "        \"id\": username,\n",
    "        \"username\": username,\n",
    "        \"status\": \"ok\",\n",
    "        \"detalhes\": \"Falhas zeradas\",\n",
    "        \"timestamp\": now_iso()\n",
    "    }\n",
    "    append_log(entry)\n",
    "\n",
    "# ============================\n",
    "# Funções para log de processamento\n",
    "# ============================\n",
    "def add_processing(username):\n",
    "    entry = {\n",
    "        \"sessao\": \"processing\",\n",
    "        \"evento\": \"add\",\n",
    "        \"id\": username,\n",
    "        \"username\": username,\n",
    "        \"status\": \"processing\",\n",
    "        \"detalhes\": \"Em processamento\",\n",
    "        \"timestamp\": now_iso()\n",
    "    }\n",
    "    append_log(entry)\n",
    "\n",
    "def remove_processing(username):\n",
    "    entry = {\n",
    "        \"sessao\": \"processing\",\n",
    "        \"evento\": \"remove\",\n",
    "        \"id\": username,\n",
    "        \"username\": username,\n",
    "        \"status\": \"done\",\n",
    "        \"detalhes\": \"Removido do processamento\",\n",
    "        \"timestamp\": now_iso()\n",
    "    }\n",
    "    append_log(entry)\n",
    "\n",
    "def is_processing(username):\n",
    "    logs = query_logs(sessao=\"processing\", id=username)\n",
    "    if not logs:\n",
    "        return False\n",
    "    last = logs[-1]\n",
    "    return last.get(\"status\") == \"processing\"\n",
    "\n",
    "# ================================================================\n",
    "# Funções de Busca de Transmissões\n",
    "# ================================================================\n",
    "def buscar_transmissoes_base(url, params=None):\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        # Garante que retorna a lista correta de transmissões (padrão da API XCam)\n",
    "        if isinstance(data, dict) and \"broadcasts\" in data and \"items\" in data[\"broadcasts\"]:\n",
    "            return data[\"broadcasts\"][\"items\"]\n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ Erro ao buscar transmissões da API: {e}\")\n",
    "        return []\n",
    "\n",
    "def validar_e_preparar_stream(stream_data, temp_folder=TEMP_OUTPUT_FOLDER):\n",
    "    username = stream_data.get(\"username\")\n",
    "    if not username:\n",
    "        print(\"⚠️ Stream sem username, ignorando.\")\n",
    "        return None\n",
    "    if is_processing(username):\n",
    "        return None\n",
    "    if is_in_blacklist(username):\n",
    "        return None\n",
    "    poster_url = stream_data.get(\"poster\")\n",
    "    poster_temp_path = None\n",
    "    if poster_url:\n",
    "        poster_temp_path = download_and_save_poster(poster_url, username, temp_folder)\n",
    "    if not is_poster_valid(poster_temp_path) and stream_data.get(\"src\"):\n",
    "        poster_temp_path = generate_poster_with_ffmpeg(stream_data[\"src\"], username, temp_folder)\n",
    "    if not is_poster_valid(poster_temp_path):\n",
    "        register_failure(username)\n",
    "        return None\n",
    "    stream_data[\"poster_temp_path\"] = poster_temp_path\n",
    "    return stream_data\n",
    "\n",
    "def get_broadcasts(limit=LIMIT_DEFAULT):\n",
    "    print(f\"⏳ Buscando até {limit} transmissões ativas na API...\")\n",
    "    streams_api = buscar_transmissoes_base(\"https://api.xcam.gay/?limit=1000\", {\"limit\": 1000})\n",
    "    if not streams_api or not isinstance(streams_api, list):\n",
    "        print(\"❌ Falha ao obter transmissões da API principal.\")\n",
    "        return []\n",
    "    valid_streams = []\n",
    "    for stream in streams_api:\n",
    "        if len(valid_streams) >= limit:\n",
    "            break\n",
    "        prepared_stream = validar_e_preparar_stream(stream)\n",
    "        if prepared_stream:\n",
    "            valid_streams.append(prepared_stream)\n",
    "        elif isinstance(stream, dict) and not stream.get(\"src\"):\n",
    "            username = stream.get(\"username\")\n",
    "            if username:\n",
    "                print(f\"ℹ️ 'src' ausente para '{username}' na API principal, tentando fallback via /liveInfo...\")\n",
    "                stream_liveinfo = buscar_transmissoes_base(f\"https://api.xcam.gay/user/{username}/liveInfo\")\n",
    "                if isinstance(stream_liveinfo, dict) and stream_liveinfo.get(\"src\"):\n",
    "                    prepared_stream_liveinfo = validar_e_preparar_stream(stream_liveinfo)\n",
    "                    if prepared_stream_liveinfo:\n",
    "                        valid_streams.append(prepared_stream_liveinfo)\n",
    "                else:\n",
    "                    register_failure(username)\n",
    "    print(f\"✅ Busca concluída. Encontradas {len(valid_streams)} transmissões válidas para processamento.\")\n",
    "    return valid_streams\n",
    "\n",
    "def buscar_usuarios_especificos(usernames, limit=API_SEARCH_LIMIT):\n",
    "    print(f\"⏳ Buscando transmissões específicas para: {', '.join(usernames)}\")\n",
    "    valid_streams = []\n",
    "    for username in usernames:\n",
    "        if len(valid_streams) >= limit:\n",
    "            break\n",
    "        print(f\"⏳ Buscando informações para usuário específico: {username}\")\n",
    "        stream_info = buscar_transmissoes_base(f\"https://api.xcam.gay/user/{username}/liveInfo\")\n",
    "        if isinstance(stream_info, dict):\n",
    "            prepared_stream = validar_e_preparar_stream(stream_info)\n",
    "            if prepared_stream:\n",
    "                valid_streams.append(prepared_stream)\n",
    "                print(f\"✅ Transmissão específica válida encontrada: {username} ({len(valid_streams)}/{limit})\")\n",
    "        else:\n",
    "            print(f\"❌ Não foi possível obter informações para o usuário específico: {username}\")\n",
    "    print(f\"✅ Busca específica concluída. Encontradas {len(valid_streams)} transmissões válidas.\")\n",
    "    return valid_streams\n",
    "\n",
    "def buscar_proxima_transmissao_livre():\n",
    "    streams_api = buscar_transmissoes_base(\"https://api.xcam.gay/?limit=1000\", {\"limit\": 20})\n",
    "    if not streams_api or not isinstance(streams_api, list):\n",
    "        return None\n",
    "    for stream in streams_api:\n",
    "        prepared_stream = validar_e_preparar_stream(stream)\n",
    "        if prepared_stream:\n",
    "            return prepared_stream\n",
    "        elif isinstance(stream, dict) and not stream.get(\"src\"):\n",
    "            username = stream.get(\"username\")\n",
    "            if username:\n",
    "                stream_liveinfo = buscar_transmissoes_base(f\"https://api.xcam.gay/user/{username}/liveInfo\")\n",
    "                if isinstance(stream_liveinfo, dict) and stream_liveinfo.get(\"src\"):\n",
    "                    prepared_stream_liveinfo = validar_e_preparar_stream(stream_liveinfo)\n",
    "                    if prepared_stream_liveinfo:\n",
    "                        return prepared_stream_liveinfo\n",
    "    return None\n",
    "\n",
    "# ================================================================\n",
    "# FIM DO ARQUIVO UNIFICADO\n",
    "# ================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jGFyqOUoKEF7",
   "metadata": {
    "id": "jGFyqOUoKEF7"
   },
   "source": [
    "# Célula 7: Gravação da Stream, Poster Automático, Controle de Falhas, Log Seguro e Blacklist Inteligente\n",
    "\n",
    "**Objetivo:**  \n",
    "Automatizar a gravação de transmissões ao vivo com ffmpeg, garantindo robustez, rastreabilidade e integração com a lógica de blacklist temporária e controle de falhas. A célula também assegura o gerenciamento seguro do log de transmissões em processamento e a limpeza de arquivos temporários.\n",
    "\n",
    "## Estratégia e melhorias implementadas\n",
    "\n",
    "- **Gerenciamento seguro de log:**  \n",
    "  O usuário é registrado no log de transmissões em processamento antes da gravação e removido dele ao final (tanto em sucesso quanto em erro), evitando duplicidade e permitindo paralelismo seguro.\n",
    "- **Poster sempre válido:**  \n",
    "  O sistema tenta baixar o poster da API. Se o poster estiver ausente, inválido ou nulo, gera automaticamente uma imagem via ffmpeg, assegurando que toda transmissão tenha um poster associado e válido.\n",
    "- **Controle de tempo mínimo:**  \n",
    "  Se a gravação resultar em vídeo muito curto, tanto o arquivo de vídeo quanto o poster são descartados imediatamente, e uma falha é registrada para o usuário.\n",
    "- **Tratamento robusto de falhas:**  \n",
    "  Qualquer falha (ffmpeg, exceptions, etc.) é registrada. Ao atingir o número máximo de falhas consecutivas (`BLACKLIST_MAX_FAILURES`), o usuário entra automaticamente na blacklist temporária, evitando tentativas infinitas e desperdício de recursos.\n",
    "- **Limpeza automatizada:**  \n",
    "  Após upload ou erro, todos os arquivos temporários (vídeo e poster) são removidos, otimizando o uso do disco e mantendo o ambiente do Colab limpo.\n",
    "- **Reset de falhas em caso de sucesso:**  \n",
    "  Quando a gravação é válida, o contador de falhas do usuário é limpo, evitando blacklist indevida.\n",
    "- **Comentários detalhados e código modular:**  \n",
    "  O fluxo é completamente documentado, facilitando manutenção, revisão e entendimento por toda a equipe.\n",
    "\n",
    "---\n",
    "\n",
    "## Fluxo resumido da função principal\n",
    "\n",
    "1. **Registra o usuário** no log de transmissões em processamento.\n",
    "2. **Garante um poster válido** (download ou geração automática).\n",
    "3. **Executa o ffmpeg** para gravar a transmissão e monitora o progresso em tempo real.\n",
    "4. **Valida a gravação**:\n",
    "   - Se falhar, registra falha e trata blacklist.\n",
    "   - Se for curta demais, descarta e registra falha.\n",
    "   - Se for válida, limpa contador de falhas e prossegue normalmente.\n",
    "5. **Após upload ou erro**, remove o usuário do log e limpa arquivos temporários.\n",
    "\n",
    "---\n",
    "\n",
    "## Exemplo de uso\n",
    "\n",
    "```python\n",
    "resultado = gravar_stream(username=\"user123\", m3u8_url=\"https://cdn.xcam.gay/m3u8/...\", poster_url=\"https://api.xcam.gay/poster/...\")\n",
    "if resultado['upload_success']:\n",
    "    print(\"Gravação e upload realizados com sucesso!\")\n",
    "else:\n",
    "    print(\"Falha na gravação ou upload:\", resultado['abyss_response'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Segurança, rastreabilidade e integração\n",
    "\n",
    "- **Pronto para CI/CD e execução paralela:**  \n",
    "  Controle rigoroso de log e blacklist garante execução concorrente, segura e rastreável por todo o pipeline XCam.\n",
    "- **Integração total com as funções globais:**  \n",
    "  Utiliza funções de blacklist e falha da Célula 6, promovendo rastreabilidade e controle centralizado.\n",
    "- **Diagnóstico facilitado:**  \n",
    "  Mensagens e logs detalhados em cada etapa do processo.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eJ_jrfNgKZNr",
   "metadata": {
    "id": "eJ_jrfNgKZNr"
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Célula 7: Gravação Automática de Transmissão, Controle de Log, Limpeza e Blacklist Inteligente\n",
    "# ================================================================\n",
    "# Objetivo:\n",
    "# - Gravar transmissões ao vivo utilizando ffmpeg, com controle rigoroso de log de processamento, tratamento de falhas e integração com blacklist temporária.\n",
    "# - Garantir que cada transmissão seja registrada no log de processamento no início e removida ao final (sucesso ou erro), evitando duplicidade ou processamento concorrente.\n",
    "# - Registrar falhas (ffmpeg, duração insuficiente, poster inválido), escalando usuários para a blacklist temporária ao atingir o limite de tentativas, conforme regras globais (Célula 6).\n",
    "# - Limpar arquivos temporários após uso.\n",
    "# - Modular e pronto para integração com pipelines CI/CD, concorrência e integração total ao pipeline XCam.\n",
    "# ================================================================\n",
    "\n",
    "def get_video_duration(filepath):\n",
    "    \"\"\"\n",
    "    Retorna a duração real do arquivo mp4, em segundos, utilizando ffprobe.\n",
    "    Retorna None em caso de erro ou se o arquivo não existir.\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    import json\n",
    "    try:\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"⚠️ Arquivo para ffprobe não encontrado: {filepath}\")\n",
    "            return None\n",
    "        cmd = [\n",
    "            \"ffprobe\", \"-v\", \"error\",\n",
    "            \"-show_entries\", \"format=duration\",\n",
    "            \"-of\", \"json\",\n",
    "            filepath\n",
    "        ]\n",
    "        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        info = json.loads(result.stdout)\n",
    "        duration = float(info[\"format\"][\"duration\"])\n",
    "        return int(round(duration))\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Não foi possível obter duração via ffprobe para {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "def gravar_stream(username, m3u8_url, poster_url=None, poster_frame_time=7):\n",
    "    \"\"\"\n",
    "    Grava a transmissão ao vivo do usuário usando ffmpeg, com controle de erros, log e integração à blacklist.\n",
    "    - Adiciona usuário ao log de processamento no início.\n",
    "    - Remove do log ao finalizar, independentemente do resultado (robusto via finally).\n",
    "    - Em caso de falha do ffmpeg ou gravação muito curta, registra falha do usuário.\n",
    "    - Ao atingir N falhas consecutivas, usuário entra na blacklist (funções globais).\n",
    "    - Limpa arquivos temporários ao final.\n",
    "    - Garante poster válido: baixa da poster_url, ou gera automaticamente com ffmpeg se ausente/inválido.\n",
    "    - poster_frame_time: segundo do vídeo onde a captura do poster será feita, se necessário.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        add_processing(username)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro ao registrar transmissão em processamento no log: {e}\")\n",
    "\n",
    "    start_time_dt = datetime.now()\n",
    "    data_str = start_time_dt.strftime(\"%d-%m-%Y\")\n",
    "    horario_str = start_time_dt.strftime(\"%H-%M\")\n",
    "    temp_filename = f\"{username}_{start_time_dt.strftime('%Y%m%d_%H%M%S')}_temp.mp4\"\n",
    "    filepath = os.path.join(TEMP_OUTPUT_FOLDER, temp_filename)\n",
    "\n",
    "    print(f\"\\n🎬 Iniciando gravação de: {username} (URL: {m3u8_url}) em {filepath}\")\n",
    "\n",
    "    # Garante poster válido\n",
    "    poster_temp_path = None\n",
    "    if poster_url:\n",
    "        poster_temp_path = download_and_save_poster(poster_url, username, TEMP_OUTPUT_FOLDER)\n",
    "    if not is_poster_valid(poster_temp_path) and m3u8_url:\n",
    "        poster_temp_path = generate_poster_with_ffmpeg(m3u8_url, username, TEMP_OUTPUT_FOLDER, frame_time=poster_frame_time)\n",
    "\n",
    "    ffmpeg_cmd = [\n",
    "        \"ffmpeg\", \"-i\", m3u8_url,\n",
    "        \"-t\", str(RECORD_SECONDS),\n",
    "        \"-c\", \"copy\", \"-y\", filepath\n",
    "    ]\n",
    "\n",
    "    start_time_process = time.time()\n",
    "    process = None\n",
    "\n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            ffmpeg_cmd,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "\n",
    "        # Monitoramento de progresso do ffmpeg (logs em tempo real)\n",
    "        elapsed_seconds = 0\n",
    "        last_log_minute = -1\n",
    "        while True:\n",
    "            line = process.stdout.readline()\n",
    "            if not line and process.poll() is not None:\n",
    "                break\n",
    "            if \"time=\" in line:\n",
    "                try:\n",
    "                    match = re.search(r\"time=(\\d+):(\\d+):(\\d+)\", line)\n",
    "                    if match:\n",
    "                        h, m, s = map(int, match.groups())\n",
    "                        elapsed_seconds = h * 3600 + m * 60 + s\n",
    "                        if elapsed_seconds // 60 != last_log_minute:\n",
    "                            log_progress(username, elapsed_seconds, RECORD_SECONDS)\n",
    "                            last_log_minute = elapsed_seconds // 60\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        process.wait()\n",
    "        end_time_process = time.time()\n",
    "        elapsed_seconds_proc = round(end_time_process - start_time_process)\n",
    "        log_progress(username, elapsed_seconds_proc, RECORD_SECONDS)\n",
    "\n",
    "        # Se FFmpeg falhou, registra falha para o usuário e retorna erro\n",
    "        if process.returncode != 0:\n",
    "            print(f\"❌ FFmpeg falhou para {username}. Código de saída: {process.returncode}\")\n",
    "            register_failure(username)\n",
    "            return {\n",
    "                'username': username,\n",
    "                'filename': temp_filename,\n",
    "                'filepath': filepath,\n",
    "                'upload_success': False,\n",
    "                'abyss_response': \"Gravação FFmpeg falhou\"\n",
    "            }\n",
    "\n",
    "        # Validação pelo tempo real do arquivo gravado (robusta)\n",
    "        elapsed_seconds_real = get_video_duration(filepath)\n",
    "        if elapsed_seconds_real is not None:\n",
    "            print(f\"✅ Duração real do arquivo gravado: {elapsed_seconds_real}s (ffprobe)\")\n",
    "        else:\n",
    "            print(f\"⚠️ Não foi possível aferir duração real, usando a do processo: {elapsed_seconds_proc}s\")\n",
    "            elapsed_seconds_real = elapsed_seconds_proc\n",
    "\n",
    "        if elapsed_seconds_real < RECORD_SECONDS_MIN:\n",
    "            print(f\"⏩ Duração gravada ({elapsed_seconds_real}s) menor que o mínimo ({RECORD_SECONDS_MIN}s). Arquivo descartado.\")\n",
    "            register_failure(username)\n",
    "            if os.path.exists(filepath): os.remove(filepath)\n",
    "            if poster_temp_path and os.path.exists(poster_temp_path): os.remove(poster_temp_path)\n",
    "            return {\n",
    "                'username': username,\n",
    "                'filename': temp_filename,\n",
    "                'filepath': filepath,\n",
    "                'upload_success': False,\n",
    "                'abyss_response': \"Gravação muito curta (descartada)\"\n",
    "            }\n",
    "\n",
    "        # Sucesso: limpa falhas acumuladas do usuário\n",
    "        clear_failure(username)\n",
    "        tempo_formatado = format_seconds(elapsed_seconds_real)\n",
    "        final_filename = f\"{username}_{data_str}_{horario_str}_{tempo_formatado}.mp4\"\n",
    "        final_filepath = os.path.join(TEMP_OUTPUT_FOLDER, final_filename)\n",
    "\n",
    "        try:\n",
    "            os.rename(filepath, final_filepath)\n",
    "            print(f\"✅ Arquivo renomeado para: {final_filename}\")\n",
    "            filepath_for_upload = final_filepath\n",
    "            filename_for_upload = final_filename\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erro ao renomear arquivo {temp_filename} para {final_filename}: {e}\")\n",
    "            filepath_for_upload = filepath\n",
    "            filename_for_upload = temp_filename\n",
    "\n",
    "        # Realiza upload e atualização do banco de dados (json)\n",
    "        success, abyss_resp, slug = upload_to_abyss_and_update_json(\n",
    "            filepath_for_upload, username, elapsed_seconds_real,\n",
    "            poster_temp_path=poster_temp_path\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'username': username,\n",
    "            'filename': filename_for_upload,\n",
    "            'filepath': filepath_for_upload,\n",
    "            'upload_success': success,\n",
    "            'abyss_response': abyss_resp,\n",
    "            'slug': slug\n",
    "        }\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Erro: Comando 'ffmpeg' não encontrado. Certifique-se de que foi instalado corretamente.\")\n",
    "        register_failure(username)\n",
    "        return {\n",
    "            'username': username,\n",
    "            'filename': None,\n",
    "            'filepath': None,\n",
    "            'upload_success': False,\n",
    "            'abyss_response': \"Comando FFmpeg não encontrado\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro inesperado durante a execução do FFmpeg para {username}: {e}\")\n",
    "        register_failure(username)\n",
    "        return {\n",
    "            'username': username,\n",
    "            'filename': None,\n",
    "            'filepath': None,\n",
    "            'upload_success': False,\n",
    "            'abyss_response': f\"Erro inesperado na execução do FFmpeg: {e}\"\n",
    "        }\n",
    "    finally:\n",
    "        # Remoção segura do usuário do log de transmissões em processamento\n",
    "        try:\n",
    "            remove_processing(username)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erro ao remover transmissão do log de processamento: {e}\")\n",
    "\n",
    "        # Limpeza do arquivo de vídeo pós-upload\n",
    "        if 'filepath_for_upload' in locals() and os.path.exists(filepath_for_upload):\n",
    "            try:\n",
    "                os.remove(filepath_for_upload)\n",
    "                print(f\"🗑️ Arquivo de vídeo removido do Colab: {filepath_for_upload}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Não foi possível remover o arquivo de vídeo temporário: {e}\")\n",
    "\n",
    "        # Limpeza do poster temporário\n",
    "        if poster_temp_path and os.path.exists(poster_temp_path):\n",
    "            try:\n",
    "                os.remove(poster_temp_path)\n",
    "                print(f\"🗑️ Poster temporário removido: {poster_temp_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Não foi possível remover o poster temporário: {e}\")\n",
    "\n",
    "# ================================================================\n",
    "# FIM DA CÉLULA 7 — Gravação, Log e Blacklist Inteligente\n",
    "# ================================================================\n",
    "\n",
    "# Observações e recomendações:\n",
    "# - Use sempre as funções globais de blacklist/falha da Célula 6 para máxima rastreabilidade.\n",
    "# - Mensagens claras e detalhadas facilitam diagnóstico, CI/CD e manutenção.\n",
    "# - Pronto para execução concorrente e integração total com pipeline modular do XCam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4acb89",
   "metadata": {
    "id": "4f4acb89"
   },
   "source": [
    "# Célula 8: Upload para Abyss.to, Atualização do rec.json, Commit Poster e Sincronização com Google Drive\n",
    "\n",
    "**Objetivo:**  \n",
    "Realizar upload do vídeo gravado para Abyss.to, registrar e atualizar todos os metadados relevantes no arquivo `rec.json` do usuário, garantir a movimentação/renomeação adequada do poster e executar o commit/push automatizado de arquivos alterados, sincronizando também com o Google Drive.  \n",
    "O processo é otimizado para processamento em lote: os arquivos modificados só são enviados quando o número atingir o limiar (`COMMIT_PUSH_THRESHOLD`), promovendo eficiência e integridade do repositório, mesmo em execução paralela.\n",
    "\n",
    "---\n",
    "\n",
    "## Estratégia e melhorias implementadas\n",
    "\n",
    "- **Commit/push em lote otimizado:**  \n",
    "  Arquivos alterados são acumulados em um buffer. O commit e push são executados automaticamente apenas quando a quantidade de arquivos atinge o threshold configurado, reduzindo conflitos e otimizando o workflow CI/CD.\n",
    "- **Sincronização automática com o Google Drive:**  \n",
    "  Sempre que `rec.json` ou poster são atualizados, uma cópia é feita para o diretório correspondente do usuário no Google Drive (se disponível), garantindo redundância, persistência e facil acesso externo aos metadados e imagens.\n",
    "- **Atomicidade e segurança em concorrência:**  \n",
    "  O acesso ao buffer de commit é protegido por lock (`threading.Lock`), assegurando integridade mesmo em processamento paralelo ou múltiplos workers.\n",
    "- **Poster sempre correto e rastreável:**  \n",
    "  O poster utilizado é sempre movido/renomeado para o local definitivo e associado ao vídeo pelo slug. O caminho é sincronizado tanto no repositório quanto no Drive.\n",
    "- **Atualização robusta do rec.json:**  \n",
    "  O histórico do usuário é preenchido com todos os campos, incluindo poster, urlIframe, data, horário e tempo formatado. O padrão da estrutura JSON é rigorosamente seguido, facilitando a integração, análise e exportação dos dados.\n",
    "- **Limpeza automática de arquivos temporários:**  \n",
    "  Após mover, copiar e commitar os arquivos, os temporários são removidos, mantendo o ambiente Colab limpo e eficiente.\n",
    "\n",
    "---\n",
    "\n",
    "## Como funciona o fluxo principal\n",
    "\n",
    "1. **Faz upload do vídeo para Abyss.to** e recebe a confirmação (slug, url, urlIframe).\n",
    "2. **Move/renomeia o poster** para o local definitivo no repositório, associando ao vídeo pelo slug.\n",
    "3. **Atualiza ou cria `rec.json`** do usuário, preenchendo todos os metadados da gravação.\n",
    "4. **Adiciona arquivos alterados ao buffer de commit** (com lock para evitar concorrência).\n",
    "5. **Sincroniza** `rec.json` e poster no Google Drive, mantendo redundância e facilidade de acesso.\n",
    "6. **Executa commit/push automático em lote** ao final do processamento faz o commit/push dos arquivos restantes.\n",
    "7. **Limpa arquivos temporários** garantindo eficiência e organização do ambiente.\n",
    "\n",
    "---\n",
    "\n",
    "## Exemplo de uso recomendado\n",
    "\n",
    "```python\n",
    "# Após concluir o upload e gerar poster:\n",
    "upload_success, abyss_response, slug = upload_to_abyss_and_update_json(\n",
    "    filepath=arquivo_video,\n",
    "    username=\"usuario\",\n",
    "    duration_seconds=duracao,\n",
    "    poster_temp_path=caminho_poster_temp\n",
    ")\n",
    "\n",
    "# Ao final do processamento, para garantir commit dos arquivos restantes:\n",
    "commit_push_restantes()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Segurança, rastreabilidade e integração\n",
    "\n",
    "- **Processo compatível com execução concorrente** e pipelines CI/CD.\n",
    "- **Commit/push protegido contra condições de corrida**, garantindo atomicidade dos dados no repositório.\n",
    "- **Sincronização Drive robusta**, ideal para ambientes colaborativos ou para garantir backup.\n",
    "- **Mensagens e logs claros** facilitam manutenção, auditoria e diagnóstico rápido em todo o pipeline XCam.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed3777f",
   "metadata": {
    "id": "8ed3777f"
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Célula 9: Processamento Automático, Paralelismo e Supervisor Dinâmico com Blacklist\n",
    "# ================================================================\n",
    "# Objetivo:\n",
    "# - Controlar e orquestrar todo o pipeline do notebook, garantindo processamento contínuo, paralelo, eficiente e seguro de transmissões ao vivo.\n",
    "# - O supervisor dinâmico mantém o lote sempre cheio, respeita a blacklist temporária e o log central, e integra todas as funções críticas das células anteriores, garantindo máxima resiliência e rastreabilidade.\n",
    "#\n",
    "# Estratégia aplicada:\n",
    "# - Utiliza múltiplos processos para gravar e processar transmissões simultaneamente\n",
    "# - O supervisor monitora constantemente as vagas livres no lote e preenche em tempo real com novas transmissões válidas\n",
    "# - Consulta o log central de processamento para evitar duplicidade, mesmo em ambientes concorrentes ou paralelos\n",
    "# - Transmissões de usuários em blacklist não são tentadas novamente durante o ciclo vigente\n",
    "# - Cada etapa do processamento é registrada com timestamp, status e contexto\n",
    "# - Ao final do ciclo (ou quando atingido o threshold), todos os arquivos alterados são enviados ao repositório\n",
    "# ================================================================\n",
    "\n",
    "from multiprocessing import Process, Manager # Import Manager here\n",
    "\n",
    "def log_supervisor(msg, level=\"INFO\"):\n",
    "    \"\"\"\n",
    "    Log supervisor padronizado para todas as etapas do pipeline.\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[{timestamp}] [{level}] {msg}\")\n",
    "\n",
    "def worker(username, m3u8_url, poster_url, results):\n",
    "    \"\"\"\n",
    "    Worker dedicado: grava a stream, faz upload, atualiza rec.json/poster, integra ao log.\n",
    "    \"\"\"\n",
    "    log_supervisor(f\"Iniciando gravação: {username} | URL: {m3u8_url} | Poster: {poster_url}\", \"WORKER\")\n",
    "    result = gravar_stream(username, m3u8_url, poster_url)\n",
    "    log_supervisor(\n",
    "        f\"Finalizou gravação: {username} | Sucesso: {result.get('upload_success')} | \"\n",
    "        f\"Arquivo: {result.get('filename')} | Abyss: {result.get('abyss_response')}\", \"WORKER\")\n",
    "    results.append(result)\n",
    "\n",
    "def supervisor_dinamico(usuarios_especificos=None):\n",
    "    \"\"\"\n",
    "    Supervisor dinâmico de transmissões ao vivo:\n",
    "    - Mantém o lote de gravações sempre cheio, preenchendo vagas em tempo real.\n",
    "    - Evita duplicidade e concorrência consultando log central.\n",
    "    - Respeita blacklist temporária, não processando usuários bloqueados no ciclo vigente.\n",
    "    - Integra-se com a lógica de blacklist, commit/push automático, limpeza de recursos e log robusto.\n",
    "    - Modular e clareza, pronta para integração com pipelines CI/CD, execução concorrente e ambientes colaborativos.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determina o tamanho do lote com base no modo operacional\n",
    "    pool_size = LIMIT_DEFAULT if not usuarios_especificos else API_SEARCH_LIMIT\n",
    "    running = []\n",
    "    results = Manager().list()\n",
    "    seen_usernames = set()\n",
    "    LOG_PROCESSAMENTO_PATH = \"/content/xcam_processing.log\"\n",
    "\n",
    "    log_supervisor(f\"Supervisor dinâmico iniciado | Lote alvo: {pool_size} | Modo: {'específico' if usuarios_especificos else 'automático'}\")\n",
    "\n",
    "    def atualizar_seen_usernames():\n",
    "        \"\"\"\n",
    "        Atualiza o conjunto de usernames já processados diretamente do log central.\n",
    "        Garante robustez em ambientes concorrentes e previne duplicidade.\n",
    "        \"\"\"\n",
    "        if os.path.exists(LOG_PROCESSAMENTO_PATH):\n",
    "            with open(LOG_PROCESSAMENTO_PATH, \"r\") as f:\n",
    "                log_set = set([line.strip() for line in f if line.strip()])\n",
    "                seen_usernames.update(log_set)\n",
    "\n",
    "    def buscar_nova_transmissao():\n",
    "        \"\"\"\n",
    "        Busca uma nova transmissão livre para preencher o lote:\n",
    "        - Modo específico: busca em lista fornecida.\n",
    "        - Modo automático: busca próxima transmissão livre disponível.\n",
    "        - Sempre consulta blacklist e log central antes de lançar.\n",
    "        \"\"\"\n",
    "        atualizar_seen_usernames()  # Sempre atualiza antes de buscar\n",
    "        if usuarios_especificos:\n",
    "            candidatos = buscar_usuarios_especificos(usuarios_especificos)\n",
    "            for s in candidatos:\n",
    "                username = s[\"username\"]\n",
    "                if username not in seen_usernames and not is_in_blacklist(username):\n",
    "                    log_supervisor(f\"Nova transmissão encontrada (específico): {username}\", \"BUSCA\")\n",
    "                    return s\n",
    "            log_supervisor(\"Nenhuma transmissão específica livre encontrada (todos em blacklist/log ou offline).\", \"BUSCA\")\n",
    "            return None\n",
    "        else:\n",
    "            # Busca otimizada: tenta até 10 vezes buscar próxima transmissão livre\n",
    "            for tentativa in range(1, 11):\n",
    "                log_supervisor(f\"Buscando próxima transmissão livre: tentativa {tentativa}\", \"BUSCA\")\n",
    "                stream = buscar_proxima_transmissao_livre()\n",
    "                if stream:\n",
    "                    username = stream[\"username\"]\n",
    "                    if username not in seen_usernames and not is_in_blacklist(username):\n",
    "                        log_supervisor(f\"Nova transmissão encontrada: {username}\", \"BUSCA\")\n",
    "                        return stream\n",
    "                    else:\n",
    "                        log_supervisor(f\"Usuário {username} já processado ou em blacklist, ignorando.\", \"BUSCA\")\n",
    "            log_supervisor(\"Nenhuma transmissão livre encontrada após tentativas (todos em blacklist/log ou offline).\", \"BUSCA\")\n",
    "            return None\n",
    "\n",
    "    # ========== Fase 1: Preenchimento do lote inicial ==========\n",
    "    log_supervisor(f\"Preenchendo lote inicial com até {pool_size} transmissões...\", \"STARTUP\")\n",
    "    tentativas = 0\n",
    "    max_tentativas = 100\n",
    "    while len(running) < pool_size and tentativas < max_tentativas:\n",
    "        stream = buscar_nova_transmissao()\n",
    "        if not stream:\n",
    "            log_supervisor(\"Fim das transmissões disponíveis para preencher lote inicial.\", \"STARTUP\")\n",
    "            break\n",
    "        username = stream[\"username\"]\n",
    "        seen_usernames.add(username)\n",
    "        # Escreve no log imediatamente para evitar duplicidade em concorrência antes do .start()\n",
    "        with open(LOG_PROCESSAMENTO_PATH, \"a\") as f:\n",
    "            f.write(f\"{username}\\n\")\n",
    "        log_supervisor(f\"Lançando processo para: {username} | {len(running)+1}/{pool_size}\", \"STARTUP\")\n",
    "        p = Process(target=worker, args=(username, stream[\"src\"], stream.get(\"poster\"), results))\n",
    "        running.append(p)\n",
    "        p.start()\n",
    "        tentativas += 1\n",
    "\n",
    "    log_supervisor(f\"Lote inicial lançado com {len(running)} transmissões.\", \"STARTUP\")\n",
    "\n",
    "    # ========== Fase 2: Loop dinâmico de preenchimento contínuo ==========\n",
    "    while True:\n",
    "        antes = len(running)\n",
    "        running = [p for p in running if p.is_alive()]\n",
    "        depois = len(running)\n",
    "        if antes != depois:\n",
    "            log_supervisor(f\"{antes-depois} gravações finalizaram. Vagas livres: {pool_size-len(running)}\", \"LOOP\")\n",
    "        vagas_livres = pool_size - len(running)\n",
    "        if vagas_livres > 0:\n",
    "            for _ in range(vagas_livres):\n",
    "                stream = buscar_nova_transmissao()\n",
    "                if not stream:\n",
    "                    log_supervisor(\"Não há mais transmissões para preencher as vagas livres.\", \"LOOP\")\n",
    "                    break\n",
    "                username = stream[\"username\"]\n",
    "                seen_usernames.add(username)\n",
    "                with open(LOG_PROCESSAMENTO_PATH, \"a\") as f:\n",
    "                    f.write(f\"{username}\\n\")\n",
    "                log_supervisor(f\"Lançando nova gravação: {username} | Vaga preenchida {len(running)+1}/{pool_size}\", \"LOOP\")\n",
    "                p = Process(target=worker, args=(username, stream[\"src\"], stream.get(\"poster\"), results))\n",
    "                running.append(p)\n",
    "                p.start()\n",
    "        if not running:\n",
    "            log_supervisor(\"Todas as transmissões possíveis já foram processadas!\", \"END\")\n",
    "            break\n",
    "        log_supervisor(\n",
    "            f\"Transmissões ativas: {len(running)} | Total processadas: {len(seen_usernames)} | Buffer de resultados: {len(results)}\",\n",
    "            \"STATUS\"\n",
    "        )\n",
    "        time.sleep(2)\n",
    "\n",
    "    # ========== Fase 3: Commit/push final e encerramento ==========\n",
    "    log_supervisor(f\"Processamento dinâmico concluído! Total de transmissões gravadas/processadas: {len(results)}\", \"RESUMO\")\n",
    "    try:\n",
    "        log_supervisor(\"Realizando commit/push final dos arquivos pendentes...\", \"FINALIZACAO\")\n",
    "        commit_push_restantes()\n",
    "        log_supervisor(\"Commit/push final executado com sucesso.\", \"FINALIZACAO\")\n",
    "    except Exception as e:\n",
    "        log_supervisor(f\"Falha ao tentar commit/push final dos arquivos restantes: {e}\", \"ERRO\")\n",
    "    log_supervisor(\"Supervisor dinâmico finalizado.\", \"END\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Função principal: inicia o notebook perguntando se o usuário quer gravar transmissões específicas ou automáticas.\n",
    "    Dispara o supervisor dinâmico na modalidade selecionada.\n",
    "    \"\"\"\n",
    "    usuarios_especificos = perguntar_transmissoes_especificas()\n",
    "    log_supervisor(\"Iniciando busca e gravação de streams (supervisor dinâmico)...\", \"MAIN\")\n",
    "    supervisor_dinamico(usuarios_especificos=usuarios_especificos)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        if 'google.colab' in str(get_ipython()):\n",
    "            main()\n",
    "        else:\n",
    "            print(\"Execute main() manualmente se desejar rodar fora do Colab.\")\n",
    "    except NameError:\n",
    "        print(\"Não está rodando em Colab/IPython. Execute main() se desejar.\")\n",
    "\n",
    "# ================================================================\n",
    "# FIM DA CÉLULA 9 — Supervisor Dinâmico, Lote Cheio e Blacklist\n",
    "# ================================================================\n",
    "\n",
    "# Observações e recomendações:\n",
    "# - Toda lógica de blacklist e commit está integrada para máxima resiliência e rastreabilidade.\n",
    "# - O log central de processamento é a fonte de verdade para sincronização entre workers/processos.\n",
    "# - Modularidade, logs claros e tratamento de erro garantem manutenção e evolução seguras.\n",
    "\n",
    "# ===========================\n",
    "# Execução automática do supervisor dinâmico\n",
    "# ===========================\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ekxlUiCNxxmP",
   "metadata": {
    "id": "ekxlUiCNxxmP"
   },
   "source": [
    "# Célula extra: Commit final de pendências\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eXVBhXjsAuAY",
   "metadata": {
    "id": "eXVBhXjsAuAY"
   },
   "outputs": [],
   "source": [
    "# Célula extra: Commit final de pendências\n",
    "def commit_final_pendencias():\n",
    "    commit_buffer = getattr(upload_to_abyss_and_update_json, 'commit_buffer', [])\n",
    "    if commit_buffer:\n",
    "        print(f\"🔔 Realizando commit/push final de {len(commit_buffer)} pendências...\")\n",
    "        git_commit_and_push(commit_buffer, commit_message=\"Commit final de pendências\")\n",
    "        commit_buffer.clear()\n",
    "    else:\n",
    "        print(\"✅ Sem pendências para commit final.\")\n",
    "\n",
    "# Execute isto ao final do processamento\n",
    "# commit_final_pendencias()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "cell_execution_strategy": "setup",
   "collapsed_sections": [
    "c9hve1ySGVAs"
   ],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
